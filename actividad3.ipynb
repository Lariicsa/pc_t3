{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2db500-5916-42db-83f0-d7544892bfed",
   "metadata": {},
   "source": [
    "# Evaluación de la segmentación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613aedaa-6435-44c2-875a-432cd8a0bc4d",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c33fb1-7940-46ea-8044-f4fe573fb5c5",
   "metadata": {},
   "source": [
    "La **ground truth** es la referencia objetiva y verificada con la que se comparan las predicciones de un sistema para saber qué tan bien está funcionando.\n",
    "\n",
    "Para validar la segmentación, tiene que haber una segmentación de referencia, lo que se denomina generalmente como un **ground truth**. Este ground truth se tiene que obtener de forma manual, pintando la silueta de los objetos de interés con un color uniforme.\n",
    "\n",
    "A menudo, la obtención de estos ground truths se denomina _etiquetado manual de imágenes_ o, en este caso, _segmentación manual de imágenes_\n",
    "\n",
    "En el aprendizaje automático y el análisis de datos, la ground truth actúa como una brújula que orienta los modelos hacia la fiabilidad, la precisión y la exhaustividad. Sin la ground truth, los modelos de IA pueden descarriarse y dar lugar a aplicaciones defectuosas y decisiones inadecuadas o sesgadas.\n",
    "\n",
    "La ground truth no es estática; evoluciona con el tiempo, reflejando patrones y verdades cambiantes. Su naturaleza dinámica subraya su importancia, impulsando a los científicos e ingenieros de datos a refinar y validar continuamente sus datos de formación para que coincidan con las verdades actuales.\n",
    "\n",
    "### ¿Por qué es Importante?\n",
    "\n",
    "**Evaluación**: Se usa para comparar las predicciones del modelo con la verdad conocida y así medir precisión, recall, F1-score, etc.\n",
    "\n",
    "**Entrenamiento supervisado**: El modelo aprende ajustando sus parámetros para que sus salidas coincidan lo más posible con el ground truth.\n",
    "\n",
    "**Análisis de errores**: Ayuda a entender en qué casos el modelo falla o acierta.\n",
    "\n",
    "\n",
    "> En detección de objetos, la **ground truth** son las anotaciones correctas (hechas por humanos) sobre qué objetos hay en la imagen y dónde están. Se usa para entrenar, validar y evaluar el rendimiento de los modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cff00a-2750-4e3b-860b-fe82d0620546",
   "metadata": {},
   "source": [
    "## Split & Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38b34c-1d36-4016-a588-3f951148489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_homogeneous(region, threshold):\n",
    "    min_val, max_val = np.min(region), np.max(region)\n",
    "    return (max_val - min_val) <= threshold\n",
    "\n",
    "def split_and_merge(image, threshold):\n",
    "    def recursive_split(region):\n",
    "        rows, cols = region.shape\n",
    "        if rows <= 1 or cols <= 1:\n",
    "            return np.zeros_like(region, dtype=np.uint8)\n",
    "        if is_homogeneous(region, threshold):\n",
    "            return np.ones_like(region, dtype=np.uint8)\n",
    "        mid_row, mid_col = rows // 2, cols // 2\n",
    "        top_left = region[:mid_row, :mid_col]\n",
    "        top_right = region[:mid_row, mid_col:]\n",
    "        bottom_left = region[mid_row:, :mid_col]\n",
    "        bottom_right = region[mid_row:, mid_col:]\n",
    "        segmented_quadrants = np.zeros_like(region, dtype=np.uint8)\n",
    "        segmented_quadrants[:mid_row, :mid_col] = recursive_split(top_left)\n",
    "        segmented_quadrants[:mid_row, mid_col:] = recursive_split(top_right)\n",
    "        segmented_quadrants[mid_row:, :mid_col] = recursive_split(bottom_left)\n",
    "        segmented_quadrants[mid_row:, mid_col:] = recursive_split(bottom_right)\n",
    "        return segmented_quadrants\n",
    "\n",
    "    def merge_regions(segmented):\n",
    "        # Placeholder function for merging adjacent regions if needed\n",
    "        return segmented\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    segmented_image = recursive_split(image)\n",
    "    segmented_image = merge_regions(segmented_image)\n",
    "    return segmented_image\n",
    "\n",
    "def main():\n",
    "    file_path = \"assets/adhd_brain.jpeg\"\n",
    "    url = \"https://psychiatryonline.org/cms/10.1176/pn.39.1.0026/asset/images/sowelladhd_media1.jpeg\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
    "    except:\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(\"Error loading image.\")\n",
    "        return\n",
    "    threshold = 20  # Adjust this value as needed\n",
    "    result = split_and_merge(image, threshold)\n",
    "    cv2.imwrite('Qsegmented_image.png', result * 255)\n",
    "    cv2.imshow('Segmented Image', result * 255)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bbab2-6b27-4a3e-94e6-431d25aaf582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
