{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2db500-5916-42db-83f0-d7544892bfed",
   "metadata": {},
   "source": [
    "# Evaluación de la segmentación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613aedaa-6435-44c2-875a-432cd8a0bc4d",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c33fb1-7940-46ea-8044-f4fe573fb5c5",
   "metadata": {},
   "source": [
    "La **ground truth** es la referencia objetiva y verificada con la que se comparan las predicciones de un sistema para saber qué tan bien está funcionando.\n",
    "\n",
    "Para validar la segmentación, tiene que haber una segmentación de referencia, lo que se denomina generalmente como un **ground truth**. Este ground truth se tiene que obtener de forma manual, pintando la silueta de los objetos de interés con un color uniforme.\n",
    "\n",
    "A menudo, la obtención de estos ground truths se denomina _etiquetado manual de imágenes_ o, en este caso, _segmentación manual de imágenes_\n",
    "\n",
    "En el aprendizaje automático y el análisis de datos, la ground truth actúa como una brújula que orienta los modelos hacia la fiabilidad, la precisión y la exhaustividad. Sin la ground truth, los modelos de IA pueden descarriarse y dar lugar a aplicaciones defectuosas y decisiones inadecuadas o sesgadas.\n",
    "\n",
    "La ground truth no es estática; evoluciona con el tiempo, reflejando patrones y verdades cambiantes. Su naturaleza dinámica subraya su importancia, impulsando a los científicos e ingenieros de datos a refinar y validar continuamente sus datos de formación para que coincidan con las verdades actuales.\n",
    "\n",
    "### ¿Por qué es Importante?\n",
    "\n",
    "**Evaluación**: Se usa para comparar las predicciones del modelo con la verdad conocida y así medir precisión, recall, F1-score, etc.\n",
    "\n",
    "**Entrenamiento supervisado**: El modelo aprende ajustando sus parámetros para que sus salidas coincidan lo más posible con el ground truth.\n",
    "\n",
    "**Análisis de errores**: Ayuda a entender en qué casos el modelo falla o acierta.\n",
    "\n",
    "\n",
    "> En detección de objetos, la **ground truth** son las anotaciones correctas (hechas por humanos) sobre qué objetos hay en la imagen y dónde están. Se usa para entrenar, validar y evaluar el rendimiento de los modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cff00a-2750-4e3b-860b-fe82d0620546",
   "metadata": {},
   "source": [
    "## Split & Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd026308-f21e-4fe0-989c-0a16f61a9b48",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "Es uno de los procesos para dividir una imagen en regiones o segmentos que tienen características similares. \n",
    "\n",
    "Divide la imagen en regiones más pequeñas y luego las fusiona si cumplen ciertos criterios de similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6cd83d-9bc5-4cab-8364-a101f8ecf8c5",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217354ba-b22d-4868-ac88-3be8dd92cfad",
   "metadata": {},
   "source": [
    "#### Definimos los parámetros iniciales a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38b34c-1d36-4016-a588-3f951148489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "THRESHOLD = 300  # umbral de varianza para dividir (threshold)\n",
    "MIN_SIZE = 16    # tamaño mínimo de región \n",
    "IMAGE_TEST = 'assets/salmon.jpg'\n",
    "\n",
    "# Homogeneidad: varianza por debajo del umbral\n",
    "def is_homogeneous(region):\n",
    "    return np.var(region) < THRESHOLD\n",
    "\n",
    "# División recursiva\n",
    "def split(img, x, y, w, h):\n",
    "    region = img[y:y+h, x:x+w]\n",
    "\n",
    "    if w <= MIN_SIZE or h <= MIN_SIZE or is_homogeneous(region):\n",
    "        return [(x, y, w, h)]\n",
    "    \n",
    "    w2, h2 = w // 2, h // 2\n",
    "    regions = []\n",
    "    regions += split(img, x, y, w2, h2)\n",
    "    regions += split(img, x + w2, y, w2, h2)\n",
    "    regions += split(img, x, y + h2, w2, h2)\n",
    "    regions += split(img, x + w2, y + h2, w2, h2)\n",
    "    return regions\n",
    "\n",
    "# Fusión simple (por cercanía y valor medio)\n",
    "def merge(img, regions):\n",
    "    canvas = np.zeros_like(img)\n",
    "    for x, y, w, h in regions:\n",
    "        region = img[y:y+h, x:x+w]\n",
    "        mean_val = int(np.mean(region))\n",
    "        canvas[y:y+h, x:x+w] = mean_val\n",
    "    return canvas\n",
    "\n",
    "\n",
    "image = cv2.imread(IMAGE_TEST, cv2.IMREAD_GRAYSCALE)\n",
    "h, w = image.shape\n",
    "\n",
    "# Proceso de Split & Merge\n",
    "regions = split(image, 0, 0, w, h)\n",
    "segmented = merge(image, regions)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Imagen original\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmentación con: Split & Merge\")\n",
    "plt.imshow(segmented, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bbab2-6b27-4a3e-94e6-431d25aaf582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
